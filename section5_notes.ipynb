{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Section 5 Notes - Advanced RAG Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Be extremely careful with version dependencies, as LangGraph and LangChain are in active development, and very sensitive to versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Ingestion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution will use vector store and ChromaDB (since it's local, fast and non persistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements:\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Creates vector store and splits the documents into it\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./.chroma\",\n",
    ")\n",
    "\n",
    "# This will be our retriever, but we will have persistence here via /chroma folder\n",
    "retriever = Chroma(\n",
    "    collection_name=\"rag-chroma\",\n",
    "    persist_directory=\"./.chroma\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ").as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Definition of State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "# States in LangGraph (aka what is rememebred across the graph) - is defined as a TypedDict object\n",
    "# TypeDict will set\n",
    "\n",
    "# Documents will be a list of strings; all other properties is a string\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: bool\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side Note - Invocation Operation\n",
    "\n",
    "* Invoke has different meanings in LangGraph and LangChain.\n",
    "* Depending on the context and object, invoke has different properties and usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retriever's invoke: \"find relevant documents\"\n",
    "# docs = retriever.invoke(\"query\")\n",
    "\n",
    "# # LLM's invoke: \"generate text\"\n",
    "# response = llm.invoke(\"What is 2+2?\")\n",
    "\n",
    "# # Embeddings' invoke: \"convert text to vectors\"\n",
    "# vectors = embeddings.invoke(\"convert this to numbers\")\n",
    "\n",
    "# # Prompt template's invoke: \"fill in the template\"\n",
    "# filled = prompt.invoke({\"question\": \"What is life?\", \"context\": \"...\"})\n",
    "\n",
    "# # Parser's invoke: \"parse the output\"\n",
    "# parsed = output_parser.invoke(\"Raw text to parse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable PassThrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telling the pipeline in LG and LC to pass something or keep it th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import RunnablePassthrough\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# # WITHOUT RunnablePassthrough ❌\n",
    "# basic_chain = (\n",
    "#     retriever  \n",
    "#     | ChatPromptTemplate.from_template(\"Context: {context}\\nPlease summarize it.\")\n",
    "#     | ChatOpenAI()\n",
    "# )\n",
    "# # Flow:\n",
    "# # 1. User asks: \"Who was Einstein?\"\n",
    "# # 2. Retriever gets documents → [docs about Einstein]\n",
    "# # 3. Template only gets {context} → \"Context: [docs about Einstein]\"\n",
    "# # 4. LLM only sees the context, original question is lost!\n",
    "\n",
    "\n",
    "# # WITH RunnablePassthrough ✅\n",
    "# better_chain = (\n",
    "#     {\n",
    "#         \"context\": retriever,          # Gets relevant docs\n",
    "#         \"question\": RunnablePassthrough()  # Preserves \"Who was Einstein?\"\n",
    "#     }\n",
    "#     | ChatPromptTemplate.from_template(\"\"\"\n",
    "#         Context: {context}\n",
    "#         Question: {question}    # Can still use original question here!\n",
    "#         Please answer the question.\n",
    "#         \"\"\")\n",
    "#     | ChatOpenAI()\n",
    "# )\n",
    "# # Flow:\n",
    "# # 1. User asks: \"Who was Einstein?\"\n",
    "# # 2. Retriever gets documents → {\"context\": [docs]}\n",
    "# # 3. RunnablePassthrough keeps question → {\"question\": \"Who was Einstein?\"}\n",
    "# # 4. Template gets both → \"Context: [docs], Question: Who was Einstein?\"\n",
    "# # 5. LLM sees both context AND original question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices for piping:\n",
    "\n",
    "# Follow this structure:\n",
    "# Simple Chain\n",
    "loader | splitter | embedder\n",
    "\n",
    "# RAG Chain\n",
    "retriever | template | llm\n",
    "\n",
    "# RAG with preserved data\n",
    "{\n",
    "    \"context\": retriever,\n",
    "    \"question\": RunnablePassthrough()\n",
    "} | template | llm\n",
    "\n",
    "# Multi-source RAG\n",
    "{\n",
    "    \"web\": web_retriever,\n",
    "    \"docs\": doc_retriever,\n",
    "    \"question\": RunnablePassthrough()\n",
    "} | template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Retriever Node\n",
    "\n",
    "* Technically vector databases will handle this for you, but let's abstract this lower and create a node that does it for you, for learning prposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever will refer to the vector store\n",
    "from typing import Any, Dict\n",
    "\n",
    "# From LangGraph family\n",
    "from graph.state import GraphState\n",
    "\n",
    "# let's call the ingestion file we made and use the retriver code\n",
    "from ingestion import retriever # import from local Python file, retriever object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(GraphState) -> Dict[str, Any]: # retrieve node, takes in Graph State. Output is dir with str or any\n",
    "    print(\"---RETRIEVING FOR YOU!---\")\n",
    "    question = state[\"question\"] # one of the defined attributes\n",
    "    \n",
    "    # Use the retriever here:\n",
    "    documents = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
